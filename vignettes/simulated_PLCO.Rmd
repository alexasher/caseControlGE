---
author: Alex Asher
title: "Semiparametric Analysis of Polygenic Gene-Environment Interactions in Case-Control Studies with caseControlGE"
date: "June, 2018"
abstract: >
 \noindent Gene-environment interactions can be efficiently estimated in case-control data by methods that assume gene-environment independence in the source population, but until recently such techniques required parametric modelling of the genetic variables.  The caseControlGE package implements the methods of Stalder et. al. (2017, \emph{Biometrika}, \textbf{104}, 801-812) and Wang et. al. (2018, unpublished), which exploit the assumption of gene-environment independence without placing any assumptions on the marginal distributions of the genetic or environmental variables.  These methods are ideally suited for analyzing complex polygenic data for which parametric distributional models are not feasible.  In addition to the two estimators, the package also supplies a function to simulate case-control data and several helper functions for use on model objects.  Use of this package is illustrated by simulating and analyzing data from a case-control study of breast cancer.
output: 
  pdf_document:
    fig_caption: yes
    keep_tex: yes
    number_sections: yes
    toc: yes
    toc_depth: 3
    includes:
      in_header: floatHere.tex
    extra_dependencies:
      - float
documentclass: article
geometry: margin=0.75in
fontsize: 11pt
vignette: >
  %\VignetteIndexEntry{Simulating and Analyzing Case Control Data with caseControlGE}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
header-includes:
  - \usepackage{amsmath}
  - \usepackage{amssymb}
  - \usepackage{natbib}
  - \usepackage{caption}
  - \usepackage{setspace}
  - \usepackage{cleveref}
---

```{r setup, cache=FALSE, include=FALSE}
knitr::opts_chunk$set(collapse=TRUE, cache=1, cache.comments=FALSE, cache.path="cache/", prompt=FALSE, comment = "#>", strip.white=TRUE, warning=TRUE)
library(knitr)
library(pander)
```

\captionsetup{justification=centering, margin=0in}
\def\bbeta{\mbox{\boldmath $\beta$}}
\def\pr{\hbox{pr}}
\def\E{{\mathbf E}}

\begin{titlepage}
\end{titlepage}

\onehalfspacing


\section{Introduction}

\subsection{caseControlGE package}

The \textbf{caseControlGE} package \citep{Asher2018R} contains tools for the analysis of case-control data using \texttt{R} \citep{R2018}.  It implements the methods of \citet{Stalder2017} and \citet{Wang2018unpublished}, both of which fall under the class of semiparametric retrospective profile likelihood estimators.  These methods are the first available to exploit the assumption of gene-environment independence while treating the genetic component nonparametrically.  As such, they are well suited to replace logistic regression as the preferred method in situations where parametric distributional models are not feasible, such as in the analysis of complex polygenic data.

\textbf{caseControlGE} contains three main functions: \texttt{simulateCC}, \texttt{spmle}, and \texttt{spmleCombo}, as well as several helper functions.  \Cref{sec:simulateCC} of this paper introduces \texttt{simulateCC} in the context of simulating case-control data analogous to the data analyzed in \cite{Wang2018unpublished}.  \Cref{sec:spmle} introduces \texttt{spmle} as a tool to analyze the simulated data, and \cref{sec:spmleCombo} introduces \texttt{spmleCombo} to conduct a more efficient analysis of the simulated data.

\subsection{Background}

Case-control studies are retrospective observational studies in which the sample consists of a group of healthy subjects and a group of diseased subjects.  A crucial aspect of the case-control design is that the outcome, disease status, is known \emph{before} sampling.  The ability to deliberately oversample diseased subjects makes the case-control design cost effective, which is why it is widely popular in studies of gene-environment interactions.

Given the genetic and environmental covariates $G$ and $E$, we assume the risk of disease $D$ in the underlying population follows the model
\begin{equation}
  \pr(D=1 \mid G,X) = H\{\beta_0 + m(G,X,\bbeta)\}, \label{eq:logisticRisk}
\end{equation}
where $H(x)=\{ 1 + \exp(-x)\}^{-1}$ is the logistic distribution function and $m(G,X,\bbeta)$ is a function that describes the joint effect of $G$ and $X$ and is known up to the unspecified parameters of interest $\bbeta$.

Given the retrospective nature of case-control sampling, it is surprising that standard prospective logistic regression can be used to obtain unbiased estimates of $\bbeta$ \citep{PrenticePyke1979}.  Logistic regression requires no assumptions about the joint distribution of $G$ and $E$, but it suffers from low power when estimating $G * E$ interaction effects.  To gain efficiency, \citet{ChatterjeeCarroll2005} exploited the assumption of gene-environment independence in the source population to maximize the retrospective likelihood while profiling out the distribution of $E$.  Their method is available as the function \texttt{snp.logistic} in the \emph{Bioconductor} package \textbf{CGEN} \citep{CGEN2012}.

The method of \citeauthor{ChatterjeeCarroll2005}, and subsequent methods utilizing the same retrospective profile likelihood framework, require a parametric model for the distribution of $G$ given $E$.  This becomes difficult as the number and complexity of genetic variables in the model grows.  Capitalizing on advances in high-throughput genomics, genome-wide association studies have identified scores of SNPs associated with complex diseases such as cancers and diabetes.  Modern case-control studies of gene-environment interactions need efficient methodology that allows for a flexible and arbitrarily complex genetic component, such as multiple correlated SNPs and/or continuous polygenic risk scores.

The SPMLE method of \citet{Stalder2017} extends the retrospective profile likelihood framework of \citeauthor{ChatterjeeCarroll2005}, dispensing with the need to model $G$ parametrically.  When the population disease rate $\pi_1$ is known, the retrospective profile loglikelihood can be estimated (up to an additive constant) using just the case-control sample and without modeling the distribution of $G$.  When $\pi_1$ is unknown but the disease is rare, estimates can be obtained using the \emph{rare disease approximation} that $\pi_1 \approx 0$, which typically introduces negligible bias \citep{Stalder2017}.

\citet{Wang2018unpublished} proposed an improvement to the method of \citeauthor{Stalder2017} that increases the efficiency of the estimates with no additional assumptions.  This development relies on the observation that the method of \citeauthor{Stalder2017} removes dependence on the distribution of the genetic and environmental variables in two different fashions; by treating the genetic and environmental variables symmetrically \citeauthor{Wang2018unpublished} generate two sets of parameter estimates that are combined to generate a more efficient estimate.

\subsection{Implementation}

The semiparametric method of \citet{Stalder2017} is implemented as the function \texttt{spmle} in \textbf{caseControlGE}, detailed in \cref{sec:spmle}.  Estimating the semiparametric profile likelihood is a computationally intensive process, and significant effort was invested in speeding up calculations.  Estimation functions, including the analytic gradient and hessian, are written in C++ and compiled using \textbf{Rcpp} \citep{Eddelbuettel2013Rcpp}, providing a tremendous speedup over native \texttt{R} code.  Extensive benchmarking and code profiling was conducted, and estimation functions were written to apply matrix operations to contiguous blocks of memory whenever possible, reducing memory latency and allowing modern processors to exploit data level parallelism and perform the same operation on multiple data points simultaneously.

The estimated semiparametric likelihood is maximized using the quasi-Newton optimizer \textbf{ucminf} \citep{Nielsen2016ucminf} using starting values from logistic regression.  \texttt{ucminf} is particularly well suited for this application because it allows us to precondition the optimization with the analytic hessian, and it evaluates the gradient after each call to the objective function.  Calculating the gradient along with the likelihood adds negligible computational complexity, so we call a single C++ function to compute them both, then return them separately to \texttt{ucminf}.  This leads \texttt{ucminf} to converge in roughly half the time of the next-fastest optimizers (several of the various \texttt{R} implementations of the BFGS algorithm tie for second place).  The unmatched speed of \texttt{ucminf} means we are willing to tolerate its bugs, which include occasionally declaring convergence before actually converging.  To address this, \texttt{spmle} checks the gradient at the reported optimum and restarts the optimization if necessary (with different starting values).

Computational complexity of the asymptotic covariance estimation, which contains a sum of the form $\sum_{i=1}^{n} \: \sum_{j=1}^{n} \: \sum_{k=1}^{n} \partial {\cal L}_{ijk}(\Omega) / \partial \Omega$, was reduced from $O(n^3)$ to $O(n^2)$ by storing intermediate values in a three-dimensional array.  This increases speed at the cost of memory usage, which climbs from $O(n)$ to $O(n^2)$, setting a practical limit on sample size in the low tens of thousands for average personal computers.  This is sufficient to analyze all but the largest case-control studies; covariance estimates for larger studies should be computed using the bootstrap.

Asymptotic covariance estimates for the Symmetric Combination Estimator of \citeauthor{Wang2018unpublished} converge slowly and unreliable in practice, often providing poor coverage.  \citeauthor{Wang2018unpublished} recommend a balanced bootstrap, with cases and controls resampled separately, to estimate covariance.  \textbf{caseControlGE} offers users with multicore computers the option to speed up computation by using multiple processors.  Parallelization is implemented using the \texttt{R} base package \textbf{parallel}, which is installed by default on all operating systems.  Parallelization on computers running Linux or macOS is done by forking the active \texttt{R} session, saving time and memory.  This option is unavailable in Windows, so parallelization is fractionally slower because a PSOCK cluster is created with a new instance of \texttt{R} running on each core.


\section{Simulating case-control data with simulateCC} \label{sec:simulateCC}

\subsection{Data description}

\cite{Wang2018unpublished} demonstrate the utility of their method by analyzing data from a case-control study of breast cancer.  This case-control sample is taken from a large prospective cohort at the National Cancer Institute: the Prostate, Lung, Colorectal and Ovarian (PLCO) cancer screening trial \citep{canzian2010comprehensive}.  The case-control study analyzed by \citeauthor{Wang2018unpublished} consists of 658 cases and 753 controls sampled from a cohort of 64,440 non-Hispanic, white women aged 55 to 74, of whom 3.72\% developed breast cancer \citep{pfeiffer2013risk}.  The data are available from the National Cancer Institute via a data transfer agreement, but cannot be distributed with the \textbf{caseControlGE} package.  Fortunately, we can use the \textbf{caseControlGE} function \texttt{simulateCC} to generate a similar data set for analysis.

Each of the 1411 subjects in the PLCO sample was genotyped for 21 SNPs that have been previously associated with breast cancer based on large genome-wide association studies.  These SNPs were weighted by their log-odds-ratio coefficients and summed to define a polygenic risk score (PRS).  A standardized version of this PRS, with mean zero and standard deviation one, was used as the genetic risk factor $G$ by \citeauthor{Wang2018unpublished}.  Early menarche is a known risk factor for breast cancer, and \citeauthor{Wang2018unpublished} used a binary indicator of whether the subject underwent early menarche as $E$ (age at menarche < 14).  Several environmental variables were recorded as part of the PLCO study, including body mass index (BMI).  There is some evidence that obese women have a reduced risk of breast cancer, so in our simulation we will consider BMI in addition to the variables modeled by \citeauthor{Wang2018unpublished}.

\subsection{Data simulation}

Genetic variables generated by \texttt{simulateCC} include SNPs and three distributions of continuous PRS: Normal(0,1), Gamma(shape=20, scale=20), and bimodal.  Environmental variables can be binary or Normal(0,1).  To simulate case-control data with \texttt{simulateCC}, we specify distributions for $G$ and $E$ and provide regression coefficients $\bbeta$ and intercept $\beta_0$ from \cref{eq:logisticRisk}.  The function \texttt{simulateCC} generates values of $G$ and $E$ for a simulated population, then simulates binary $D$ from its conditional distribution $(D \mid G, X, \beta_0, \bbeta)$.  A sample of $n_1$ cases and $n_0$ controls is taken from this simulated population.

To determine the appropriate distributions to use when simulating $G$ and $E$, we examine the PLCO data.  In doing so, it is important to keep in mind that the case control sample is not representative of the source population.  Case-control studies deliberately oversample cases, so the distribution of $G$ and $E$ in the sample may be quite different from the distribution of $G$ and $E$ in the population (especially for variables that are strongly correlated with disease status).  To accurately simulate the genetic and environmental variables from the PLCO study, we need to estimate their distributions \emph{in the source population}.

\citeauthor{Wang2018unpublished} report $\beta_G = 0.459$ with $p < 1e-4$, but they standardized $G$ to mean zero and standard deviation one \emph{in the case-control sample}.  $G$ has a strong positive effect on disease risk, indicating that the distribution of $(G|D=0)$ is meaningfully different from the distribution of $(G|D=1)$.  Specifically, $\mathbf{E}(G|D=1) > \mathbf{E}(G|D=0)$.  With a population disease rate of 0.0372, this implies $\mathbf{E_{\rm pop}}(G) \approx \mathbf{E}(G|D=0) < 0$, where the subscript pop emphasizes that the expectation is in the source population.

This causes no problem for \citeauthor{Wang2018unpublished}, but it presents us with the dilemma that $G \nsim \hbox{N}(0,1)$.  If we simulate $G \sim \hbox{N}(0,1)$ and use $\beta_G = 0.459$ as reported in \citeauthor{Wang2018unpublished}, our simulated $(D \mid G, X, \beta_0, \bbeta)$ will not match the distribution of the actual PLCO data.

If we did not have access to the PLCO data, our best option would be to approximate $\delta = \mathbf{E}(G|D=0)$, simulate $G \sim \hbox{N}(\delta, 1)$, and use $\bbeta$ as reported in \citeauthor{Wang2018unpublished}.  While we cannot distribute the PLCO data, we \emph{can} use it to estimate population parameters, so approximating $\delta$ is not necessary.  The simplest and most common way to estimate population parameters is to calculate them using just the controls.  Case-control designs are typically used to study relatively rare diseases, and the bias introduced by using the cases as a stand-in for the population is usually quite small.

When $\pi_1$ is known, it is possible to calculate unbiased estimates by weighting the cases and controls by $\pi_1$ and $(1 - \pi_1)$, respectively.  (This technique is employed to great effect by \citeauthor{Stalder2017}, and is the reason that \texttt{spmle} requires the user to specify a value for \texttt{pi1}.)

We return to the PLCO data to conduct an analysis similar to that of \citeauthor{Wang2018unpublished}, but with two environmental variables: the indicator of early menarche and BMI.  We will standardize the two continuous variables due to their very different scales, but to make our lives easier when we conduct subsequent simulations, we standardize them to have mean zero and standard deviation one \emph{in the source population}.

We calculate $\mathbf{\widehat{E}_{\rm pop}}(\rm PRS)$, $\mathbf{\widehat{E}_{\rm pop}}(\text{early menarche})$, and $\mathbf{\widehat{E}_{\rm pop}}(\rm BMI)$ by weighting the means within cases and controls by $\pi_1$ and $(1 - \pi_1)$, respectively.  We calculate $\widehat{\hbox{sd}}_{\rm pop}(\rm PRS)$ and $\widehat{\hbox{sd}}_{\rm pop}(\rm BMI)$ using the sample standard deviations among the controls only.  We have
\vspace{-5mm}
\begin{align*}
  G = \frac{{\rm PRS} - \mathbf{\widehat{E}_{\rm pop}}(\rm PRS)}{\widehat{\hbox{sd}}_{\rm pop}(\rm PRS)}, && E_1 = \mathbf{I}(\texttt{age at menarche} < 14), && E_2 = \frac{{\rm BMI} - \mathbf{\widehat{E}_{\rm pop}}(\rm BMI)}{\widehat{\hbox{sd}}_{\rm pop}(\rm BMI)}.
\end{align*}
After this scaling, the distributions of $G$ and $E_2$ in the source population can be well approximated by uncorrelated $\hbox{N}(0, 1)$ random variables.  Binary environmental variable $E_1$ is also uncorrelated with $G$, and has a frequency of 0.745 in the population.  We fit a model in these variables to the PLCO data using \texttt{spmleCombo}, yielding the rest of the information we need to simulate case control data:
\vspace{-5mm}
\begin{align*}
  \pi_1   & =    0.0372        & n_0          & =    753               & n_1          & =     658          \\
  G       & \sim \hbox{N}(0,1) & E_1          & \sim \hbox{Bin}(0.745) & E_2          & \sim \hbox{N}(0,1) \\
  \beta_G & =    0.450         & \beta_{E_1}  & =    0.143             & \beta_{E_2}  & =    -0.019        \\
          &                    & \beta_{GE_1} & =   -0.195             & \beta_{GE_2} & =    -0.040
\end{align*}
The logistic intercept $\beta_0$ is not consistently estimated by logistic regression or either of the semiparametric methods in \textbf{caseControlGE}, however it is typically of little interest.  The function \texttt{simulateCC} prints the population disease rate each time it runs, so we run \texttt{simulateCC} several times with different values of $\beta_0$.  Using a guess-and-check approach with increasing sample size as we get closer, we manipulate $\beta_0$ to match the disease rate observed in the source population.

```{r}
#### Load the castControlGE package and set the random seed
library("caseControlGE")
set.seed(979)

#### Generate data with beta0 = -3 as a starting point
tmp = simulateCC(ncase=1000, ncontrol=1000, beta0 = -3, betaG_normPRS=0.450,
                 betaE_bin=0.143, betaE_norm=-0.019, betaGE_normPRS_bin=-0.195,
                 betaGE_normPRS_norm=-0.040, E_bin_freq=0.745)
#### Disease rate too high, try beta0 = -4
tmp = simulateCC(ncase=1000, ncontrol=1000, beta0 = -4, betaG_normPRS=0.450,
                 betaE_bin=0.143, betaE_norm=-0.019, betaGE_normPRS_bin=-0.195,
                 betaGE_normPRS_norm=-0.040, E_bin_freq=0.745)
#### Continue guessing, increasing sample size as we get closer (not run)
## tmp = simulateCC(ncase=1000, ncontrol=1000, beta0 = -3.5, ...
## tmp = simulateCC(ncase=10000, ncontrol=10000, beta0 = -3.4, ...
## tmp = simulateCC(ncase=100000, ncontrol=100000, beta0 = -3.4, ...
## tmp = simulateCC(ncase=100000, ncontrol=100000, beta0 = -3.41, ...
rm(tmp)
```

After several iterations (commented out for speed), we determined that \texttt{beta0 = -3.41} produces a population disease rate of 0.0372.  Now we generate our simulated PLCO data.
```{r simulate}
#### Set the random seed for reproducability
set.seed(70)

#### Generate a synthetic data set that has similar properties to the PLCO data
dat = simulateCC(ncase=658, ncontrol=753, beta0 = -3.41, betaG_normPRS=0.450,
                 betaE_bin=0.143, betaE_norm=-0.019, betaGE_normPRS_bin=-0.195,
                 betaGE_normPRS_norm=-0.040, E_bin_freq=0.745)
```

\subsection{Confirming the G-E independence assumption}

The function \texttt{simulateCC} returns a list with elements \texttt{D}, \texttt{G}, and \texttt{E}, which are numeric vectors or matrices.  We combine them into a \texttt{data.frame} to print the first 6 rows and tabulate by disease status.

```{r dependson="simulate"}
#### Examine the simulated data and tabulate the number of cases & controls
kable(list(head(as.data.frame(dat)), table(dat$D, dnn="D")), cap="Simulated PLCO data", book=T)
```

Case-control data generated by \texttt{simulateCC} are sorted by disease status (which is why the first 6 rows do not contain a single case), but we see there are 658 cases and 753 controls.  Before we calculate the \texttt{spmle}, we will check the assumption of gene-environment independence in the source population.  In our case, this check is largely perfunctory because we did not provide the arguments \texttt{regress\_E\_bin\_on\_G\_normPRS} or \texttt{regress\_E\_norm\_on\_G\_normPRS} to \texttt{simulateCC}, so the genetic and environmental variates were drawn from independent distributions.

But when analyzing real data, it is crucial to verify this assumption.  Violations of the $G$-$E$ independence assumption can introduce bias in the estimates of interaction parameters between the specific genetic and environmental variables in violation of $G$-$E$ independence (the other parameters in the model appear unaffected in simulation studies by \citeauthor{Stalder2017}).

We do this by checking for dependence between \texttt{G} and \texttt{E} in the controls.  Environmental variable $E_1$ is binary, so we use a \emph{t}-test of $G$ over the two levels of $E_1$.  To test for dependence between $G$ and $E_2$, we conduct a correlation test.

```{r dependson="simulate"}
#### Save the indices of all controls in dat
controls = which(dat$D == 0)

#### t-test of G over the levels of E1
pander(t.test(dat$G[controls] ~ dat$E[controls, 1]), split.cells=11)
#### correlation test between G and E2
pander(cor.test(dat$G[controls], dat$E[controls, 2]))
```

Now that we are satisfied the assumption of gene-environment independence has not been violated, we can exploit this assumption using the two semiparametric retrospective methods of \textbf{caseControlGE}.

\section{Analyzing case-control data with spmle} \label{sec:spmle}

\subsection{Known and rare disease} \label{sec:spmle.knownRare}

The function \texttt{spmle} is the backbone of \textbf{caseControlGE}; it is called on its own to evaluate the SPMLE of \citeauthor{Stalder2017}, and it is the key component of the Symmetric Combination Estimator of \citeauthor{Wang2018unpublished}.  Calling \texttt{spmle} is slightly different from calling other estimation commands like \texttt{lm} or \texttt{glm} because we do not specify the model formula to \texttt{spmle}.  Instead we specify which variables are genetic and which are environmental, and \texttt{spmle} fits the formula: \texttt{D {\raise.17ex\hbox{$\scriptstyle\sim$}} G * E}.

\texttt{spmle} returns an \texttt{S3} object of class \texttt{"spmle"}.  \textbf{caseControlGE} contains \texttt{spmle} methods for all applicable \texttt{S3} generics, such as \texttt{summary.spmle}, \texttt{print.spmle}, `anova.spmle`, `predict.spmle`, \texttt{confint.spmle}, etc.  

We fit the gene-environment interaction model with \texttt{spmle}, and compare it to the estimates from standard logistic regression.  We do not need to fit the logistic regression model with a call to \texttt{glm} because \texttt{spmle} automatically fits a logistic regression model to obtain starting values.  This logistic regression model is returned with the fitted \texttt{spmle} object.
```{r full, dependson="simulate"}
#### Fit the spmle to the simulated PLCO data
spmleFull = spmle(D=D, G=G, E=E, pi1=0.0372, data=dat)
#### Print coefficient estimates from spmle and the logistic model returned by spmle
kable(summary(spmleFull)$coefficients, caption="spmle, known pi1", digits=4)
kable(summary(spmleFull$glm_fit)$coefficients, caption="logistic regression", digits=4)
```

The parameter estimates are extremely similar between the two models, but the \texttt{spmle} has smaller standard errors for the interaction terms.  Logistic regression uncovers some evidence of a \texttt{G:E1} interaction between the PRS and early menarche, but the result is not significant at the 0.05 level.  The \texttt{spmle} is able to provide stronger evidence of a \texttt{G:E1} interaction because the estimated standard error of the \texttt{G:E1} coefficient is 20\% larger with logistic regression than the \texttt{spmle}, giving a variance increase of almost 45\%.

In this instance we know the true population disease rate $\pi_1=0.0372$.  If $\pi_1$ were unknown we would calculate the \texttt{spmle} under the assumption that $\pi_1 \approx 0$.  Calculating the rare disease approximation using \texttt{spmle} is as simple as specifying \texttt{pi1 = 0} in the function call.

```{r dependson="simulate"}
kable(summary(spmle(D=D, G=G, E=E, pi1=0, data=dat))$coef, cap="spmle, rare disease", dig=4)
```

The estimates and standard errors are nearly identical to the model with known $\pi_1$, indicating that a valid estimator can be obtained even when the disease rate is unknown.

\subsection{Reduced model test} \label{sec:spmle.reduced}

Body mass index does not appear to be a significant predictor in this model.  The coefficients for the $E_2$ main effect and the $G*E_2$ interaction are near zero and both terms have large $p$ values, so we fit a reduced model without BMI.  To demonstrate the options controlling optimization, we disable hessian preconditioning and supply bad starting values to the optimizer while fitting the reduced model.
```{r reduced, dependson="simulate"}
#### Fit the reduced spmle with bad starting values
spmleRed = spmle(D=D, G=G, E=E[,1], pi1=0.0372, data=dat, 
                 startvals=rep(NA, 4), control=list(use_hess=FALSE))
```

With invalid starting values, \texttt{ucminf} is unable to converge during its first attempt.  \texttt{splme} checks whether \texttt{ucminf} has converged and, seeing that it has not, prints "\texttt{ucminf retry 1 of 2}" and restarts the optimization with different starting values.  If optimization had failed to converge on the second try, `spmle` would have issued an error.  The number of retries, convergence criterion, and other optimization parameters can be passed as elements of the `control` argument.

`summary.spmle` reports the number of retries, number of `ucminf` iterations, and maximum gradient at the optimum, so we can confirm that the model really did converge while we check the parameter estimates.

```{r dependson="reduced"}
#### Check convergence and parameter estimates
summary(spmleRed, signif.legend=FALSE)
```

We see that not only did the model converge, but it converged to nearly the same parameter estimates as the full model.  To check whether the reduced model is just as good as the model with BMI, we use `anova` to conduct a nested model test.

If the function \texttt{anova} is called on \texttt{spmle} objects, the method \texttt{anova.spmle} is used to calculate likelihood ratio tests of the models.  This is a valid way to test full vs reduced \texttt{spmle} models because the loglikelihood reported by \texttt{logLik.spmle} is accurate up to an additive constant. However, \texttt{anova} should not be used to compare an \texttt{spmle} model to a model fit by a different method.

```{r dependson=c("full", "reduced")}
#### Likelihood ratio test for reduced vs full model
pander(anova(spmleRed, spmleFull))
```

The negligible difference in loglikelihood and large *p* value of the likelihood ratio test confirms our suspicion that BMI is not an important predictor of breast cancer status in the simulated PLCO data.  Now that we have completed variable selection, we will fit the Symmetric Combination Estimator of \citeauthor{Wang2018unpublished} using early menarche as the sole environmental variable.  The Symmetric Combination Estimator is not a maximum (pseudo)likelihood estimator like `spmle`; it is the optimal combination of two such estimators.  As such, it has no associated loglikelihood and the function `anova.spmle` cannot be used to compare models fit using the Symmetric Combination Estimator.

\section{Analyzing case-control data with spmleCombo} \label{sec:spmleCombo}

\subsection{Fitting spmleCombo with bootstrap standard error estimates} \label{sec:spmleCombo.boot}

The SPMLE of \citeauthor{Stalder2017} exploits the gene-environment independence assumption to produce an estimator that is substantially more efficient than standard logistic regression.  It is remarkable then, that the Symmetric Combination Estimator of \citeauthor{Wang2018unpublished} is yet more efficient than the SPMLE without requiring any additional assumptions about the data.  But there is no free lunch, and the challenge presented by the Symmetric Combination Estimator is that its standard error converges to its asymptotic limit very slowly, making asymptotic standard error estimates imprecise and unreliable in practice.

The **caseControlGE** implementation of the Symmetric Combination Estimator, `spmleCombo`, estimates standard error using the balanced bootstrap recommended by \citeauthor{Wang2018unpublished}, wherein cases and controls are resampled separately to maintain their sample sizes.  To speed up the process, `spmleCombo` can run the bootstrap on multiple cores in parallel with the argument `ncores`.  By default, `spmleCombo` uses 50 bootstraps to estimate the standard error, executed in series on a single core.  This is typically sufficient to obtain a reasonable estimate of standard error without undue computational burden.  Users with multiple cores, small data sets, or copious free time can increase the number of bootstraps for more precise estimates, though there are diminishing returns in precision as the number of bootstraps grows large.  Here we use 100 bootstraps distributed between 2 cores.

```{r symCombo, cache=1, cache.comments=FALSE, dependson="simulate"}
#### Set seed for reproducability, then fit spmleCombo with 100 bootstraps and 2 cores
set.seed(75)
comboRed = spmleCombo(D=D, G=G, E=E.1, pi1=0.0372, 
                      data=as.data.frame(dat)[,-4], nboot=100, ncores=2)
```

When we fit the SPMLE `spmleRed` in \cref{sec:spmle.reduced}, we removed BMI from the model by specifying `E=E[,1]`.  In the code above we achieve the same effect by coercing \texttt{dat} into a \texttt{data.frame} and dropping $E_2$ from the \texttt{data} that is passed to \texttt{spmleCombo}.  We examine the parameter estimates and variance.

```{r dependson="symCombo"}
#### Print coefficient estimates for the Symmetric Combo reduced model
kable(summary(comboRed)$coefficients, caption="Symmetric Combo, known pi1", digits=4)
#### Ratio of variances: logistic regression / Symmetric Combo
pander(t(diag(vcov(comboRed$glm_fit))/diag(vcov(comboRed))), 
       cap="Ratio of variances: logistic / Symmetric Combo")
```

Parameter estimates from the Symmetric Combination Estimator are very similar to those of the SPMLE, but the efficiency is greater still.  The lack of a viable asymptotic standard error is an inconvenience, but with no additional assumptions and a compute time measured in minutes if not seconds, the Symmetric Combination takes the day.  And because the Symmetric Combination is a combination of two SPMLE models, `spmleCombo` includes both SPMLE fits in the `spmle` object it returns.  The first of which, labeled `spmle_E` because its likelihood was profiled over the distribution of $E$, is the same reduced model we fit with `spmle` in \cref{sec:spmle.reduced}.  The symmetric counterpart, which maximizes the likelihood profiled over the distribution of $G$, is returned as `spmle_G`.

```{r dependson=c("symCombo, reduced")}
#### Verify that the "spmle_E" component of comboRed is the same as spmleRed from section 3.2
all.equal(coef(comboRed$spmle_E), coef(spmleRed), check.attributes=FALSE)

#### Print coefficient estimates for the SPMLE profiled over G
kable(summary(comboRed$spmle_G)$coef, cap="spmle profiled over G", digits=4)
```

The estimates from the SPMLE profiled over $G$ bear a striking similarity to those from version profiled over $E$.  It is this correlation that makes the asymptotic standard error of the Symmetric Combination Estimator so slow to converge to its asymptotic limit.  `spmleCombo` will (grudgingly) report the asymptotic standard error estimate if we disable the bootstrap with `nboot = 0`.  This triggers a warning, which we will see when we calculate the Symmetric Combination Estimator using the rare disease approximation.

```{r asyCombo, dependson="simulate"}
#### Force spmleCombo to report the asymptotic SE for the rare disease model
comboRare = spmleCombo(D=D, G=G, E=E.1, pi1=0, data=as.data.frame(dat)[,-4], nboot=0)
```

If we *only* want point estimates of the parameters, and not their standard errors, we can ignore this warning.  But if we want *p*-values or confidence intervals, we should not put faith in the asymptotic estimates of standard error.

```{r dependson="asyCombo"}
#### Print coefficient estimates with asymptotic SE for the rare disease approximation
kable(summary(comboRare)$coefficients, caption="spmleCombo, rare : Asymptotic SE", digits=4)
```

The parameter estimates are perfectly reasonable, but the asymptotic standard error estimates for $G$ and $G * E_1$ are completely unbelievable.

\subsection{Residual analysis} \label{sec:spmleCombo.resid}

Diagnostic plots of the Pearson residuals from the SPMLE or Symmetric Combination models are interpreted similarly to diagnostic plots from logistic regression.  As with logistic regression, we do not expect normally distributed residuals.  Instead we check that the expected value of the residual is near zero over the range of fitted values.  **caseControlGE** provides the `plot.spmle` function to do just this, and draws a lowess curve through the residuals.

```{r dependson="symCombo", cache=FALSE, fig.cap="Reduced model diagnostics: residuals vs predicted values"}
#### Plot Pearson residuals vs predicted value
plot(comboRed, main="Residuals vs Fitted: Symmetric Combo, known pi1")
```

The fitted lowess curve is near zero over the range of predicted values, raising no concerns about model fit.  Below we plot residuals vs. independent variables and find no issues

```{r dependson="symCombo", cache=FALSE, fig.show="hold", out.width="50%", fig.cap="Reduced model diagnostics: residuals vs independent variables"}
#### Plot Pearson residuals vs independent variables
plot(resid(comboRed)~dat$G, ylab="residuals", xlab="PRS")
panel.smooth(x=dat$G, y=resid(comboRed))
boxplot(resid(comboRed) ~ dat$E[,1], notch=TRUE, names=c("late menarche", "early menarche"))
```

Finally, we plot the residuals against BMI, the independent variable we removed from the model after the likelihood ratio test indicated it was not improving the model.  We expect to see no pattern to the residuals.

```{r dependson="symCombo", cache=FALSE, fig.cap="Reduced model diagnostics: residuals vs BMI, excluded from the model"}
#### Plot Pearson residuals vs independent variables
plot(resid(comboRed)~dat$E[,2], ylab="residuals", xlab="BMI", main="Residuals vs BMI")
panel.smooth(x=dat$E[,2], y=resid(comboRed))
```

\subsection{Predictions} \label{sec:spmleCombo.pred}

```{r transparency, echo=FALSE}
## Add transparency, written by SachaEpskamp: github.com/SachaEpskamp/qgraph/blob/master/R/addTrans.R
addTrans = function(color, trans) {
  if(length(color)!=length(trans)&!any(c(length(color),length(trans))==1)) {stop("Vector lengths not correct")}
  if(length(color)==1 & length(trans)>1) {color = rep(color,length(trans))}
  if(length(trans)==1 & length(color)>1) {trans = rep(trans,length(color))}
  num2hex = function(x) {
    hex = unlist(strsplit("0123456789ABCDEF", split=""))
    return(paste(hex[(x-x%%16)/16+1], hex[x%%16+1], sep=""))
  }
  rgb = rbind(col2rgb(color),trans)
  res = paste("#", apply(apply(rgb, 2, num2hex), 2, paste, collapse=""), sep="")
  return(res)
}
```

Satisfied that the model fits well, our final task is to visualize the model by predicting the expected probability of developing breast cancer for subjects who experience early or late menarche, spanning the gamut of PRS values.  We use the function `predict.spmle`, which takes the same arguments as the `predict` methods for other classes.  The one detail we must keep in mind is that `spmle` and `spmleCombo` allow the `data` argument to be a `data.frame` *or* a `list`.  When we fit `comboRed` we coerced `dat` into a `data.frame`, so when we make predictions using `comboRed`, we will need to supply `newdata` as a `data.frame` with variable names "D", "G", and "E.1".

The following lengthy block of code creates two sets of prediction data: one for women who underwent early menarche and one for women who did not.  We plot the curves for both groups of women along with a 95% confidence interval of the mean (`predict.spmle` does not calculate prediction intervals for an individual because they are not easily interpretable with a binary response).

```{r dependson="symCombo", cache=FALSE, fig.cap="Predicted probability of developing breast cancer", fig.dim=c(7.5, 6)}
#### Store vectors of polygenic risk scores, separated by disease status and menarche timing
GcaseEarly = dat$G[which(dat$D==1 & dat$E[,1]==1)]
GcaseLate = dat$G[which(dat$D==1 & dat$E[,1]==0)]
GcontrolEarly = dat$G[which(dat$D==0 & dat$E[,1]==1)]
GcontrolLate = dat$G[which(dat$D==0 & dat$E[,1]==0)]

#### Create a grid of PRS values
xg = seq(from=min(dat$G), to=max(dat$G), length=100)

#### Create two sets of new data, one with early menarche, the other without
newEarly = data.frame(G=xg, E.1=1)
newLate = data.frame(G=xg, E.1=0)

#### Predict risk and get CI for both data sets
predEarly = predict(object=comboRed, newdata=newEarly, interval="confidence", type="response")
predLate = predict(object=comboRed, newdata=newLate, interval="confidence", type="response")

#### Calculate the boundaries of a 95% CI for each data set
CIearly = data.frame(x=c(xg, rev(xg)), y=c(predEarly[,"lwr"], rev(predEarly[,"upr"])))
CIlate = data.frame(x=c(xg, rev(xg)), y=c(predLate[,"lwr"], rev(predLate[,"upr"])))

#### Plot the predictions for women with early age at menarche
plot(x=xg, y=predEarly[,"fit"], xlim=range(xg), ylim=range(c(predEarly, predLate)), type="l", 
     lwd=3, col="blue", xlab="Polygenic Risk Score", ylab="Predicted pr(D=1)",
     xaxs="i", main="Effect of Polygenic Risk Score and Early Age at Menarche")

#### Add the predictions for women with late age at menarche
lines(x=xg, y=predLate[,"fit"], lwd=3, col="red")

#### Shade the 95% CIs
polygon(x=CIearly, col=addTrans("blue", 120), border=NA)
polygon(x=CIlate, col=addTrans("red", 120), border=NA)

#### Add a rug for each data set: Early in blue, Late in red, and cases above, controls below
rug(GcaseEarly, ticksize=0.02, side=3, col="blue")
rug(GcaseLate, ticksize=0.02, side=3, col="red")
rug(GcontrolEarly, ticksize=0.02, side=1, col="blue")
rug(GcontrolLate, ticksize=0.02, side=1, col="red")

#### Add a vertical line at the population mean polygenic risk score
abline(v=0, lty=3, col="darkslategray")

#### Add a legend
legend(x="topleft", inset=0.05, title="Age at Menarche", legend=c("Early", "Late"), col=c("blue", "red"), lwd=3)
```

Both early menarche and PRS have positive main effects, but the interaction has a negative coefficient.  Visualizing the model, we see how this interesting effect plays out: *on average*, women who experienced early menarche are at higher risk for breast cancer.  However, among women with a high PRS, those who underwent early menarche are at lower risk of developing breast cancer than those who underwent late menarche.  The difference in risk at the far right end of the scale appears dramatic, but examining the tic-marks indicating observed values of PRS we see that there is a lone outlier with PRS > 3, so we would be well advised to ignore or crop values of PRS above 3.

On the other end of the spectrum, women who had early menarche experience higher risk of breast cancer than women with equally "good" genes who had late menarche.

\pagebreak

\bibliographystyle{biomAbhra}
\bibliography{alexrefs}


